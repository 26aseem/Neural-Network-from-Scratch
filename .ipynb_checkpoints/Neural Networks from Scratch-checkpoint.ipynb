{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "         [4, 1, 5, 0.5],\n",
    "         [1.5, 0.5, 4, 1.5]]\n",
    "weights = [\n",
    "            [0.2, 0.8, -0.5, 1],\n",
    "            [0.6, 0.2, 0.44, -0.98],\n",
    "            [1, 0.5, 1.5, 2]\n",
    "          ]\n",
    "biases = [2,3,4]\n",
    "\n",
    "\n",
    "weights1 = [\n",
    "            [0.1, -0.8, 1],\n",
    "            [0.36, 0.52, -0.18],\n",
    "            [.5, 0.5, 1]\n",
    "          ]\n",
    "biases1 = [3,4,1]\n",
    "\n",
    "\n",
    "# output = [\n",
    "#             inputs[0]*weights[0][0] + inputs[1]*weights[0][1] + inputs[2]*weights[0][2] +inputs[3]*weights[0][3] + biases[0],\n",
    "#             inputs[0]*weights[1][0] + inputs[1]*weights[1][1] + inputs[2]*weights[1][2] +inputs[3]*weights[1][3] + biases[1],\n",
    "#             inputs[0]*weights[2][0] + inputs[1]*weights[2][1] + inputs[2]*weights[2][2] +inputs[3]*weights[2][3] + biases[2]\n",
    "#         ]\n",
    "\n",
    "# Output for current Layer\n",
    "layers_output = []\n",
    "\n",
    "# for neuron_weights, neuron_bias in zip(weights,biases):\n",
    "#     neuron_output = 0\n",
    "#     for weight,n_input in zip(neuron_weights,inputs):\n",
    "#         neuron_output += weight * n_input\n",
    "#     neuron_output += neuron_bias\n",
    "#     layers_output.append(neuron_output)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "layerwise_output1 = np.dot(inputs,np.array(weights).T) + biases\n",
    "layerwise_output2 = np.dot(layerwise_output1,np.array(weights1).T) + biases1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8   2.87 15.5 ]\n",
      " [ 1.6   7.31 17.  ]\n",
      " [ 2.2   4.29 14.75]]\n",
      "\n",
      "[[16.684   4.4304 20.335 ]\n",
      " [14.312   5.3172 22.455 ]\n",
      " [14.538   4.3678 18.995 ]]\n"
     ]
    }
   ],
   "source": [
    "# print(output)\n",
    "# print(layers_output)\n",
    "print(layerwise_output1)\n",
    "print()\n",
    "print(layerwise_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.zeros((8,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 1:\n",
      "[[ 0.01324324 -0.91169567 -0.23273137 -0.61261269  0.26396325 -0.8637974\n",
      "  -0.80333023  0.65306588 -0.28591833  0.79089332 -0.58789512  0.43981904\n",
      "   0.08597098 -0.69248589  0.14341149  0.03071687  0.45694194  0.13062628\n",
      "   0.95309266  0.13251622 -1.08337896  0.281593    0.66944199  0.07114968\n",
      "   1.49562194 -0.1296973  -0.04324535 -0.01481083 -0.34367499  0.18009069\n",
      "   0.27845843  0.92319537  0.04398116 -0.43725104 -0.3708322  -0.20611383\n",
      "   0.45631168  0.30509333  0.54613272 -0.09808128 -0.26001289  0.0289133\n",
      "  -0.32533617  0.60781484 -1.26953936  0.6235305  -0.44546532 -0.44230084\n",
      "   0.34068113  0.27474302 -0.00940158  0.11818952 -0.18342106  0.23506211\n",
      "  -0.82916472  0.72028206  0.55287303 -1.03561377  0.09322741  0.42417418]\n",
      " [-0.01657598 -0.48467548 -0.30972541 -0.7270047   0.51781417 -0.73172103\n",
      "  -0.88947351  0.39647907 -0.55497608  0.4574415   0.05779101  0.46268125\n",
      "   0.64661806 -0.82351355  0.33101419 -0.24233612  0.07543443 -0.18658553\n",
      "   0.37908514  0.01526296 -1.31613501  0.5772003   0.64231773 -0.72199896\n",
      "   1.5644178   0.42812819  0.50239404 -0.16330374 -0.30442629  0.75530908\n",
      "   0.00461263  0.92163295  0.00937045 -0.08376745 -0.09013373  0.23619861\n",
      "   0.36056766  1.10124841  0.32310561  0.0883007   0.49087251 -0.83412627\n",
      "  -0.97427365  0.97990485 -1.11919509  0.98128443 -0.52608879 -0.32627024\n",
      "   0.63803121  0.68181925  0.56195987  0.46361162 -0.5556631   0.73701349\n",
      "  -0.4742989   0.71129364  0.70548537 -0.49996265  0.21987376  0.52568463]\n",
      " [ 0.17894444 -0.50985414 -0.02580967  0.12987277  0.14884988 -0.43261655\n",
      "  -0.52959163  0.71667786 -0.09776116  0.33351861 -0.13903987  0.45228271\n",
      "   0.19612144 -0.39809511  0.00724466  0.07465171  0.24101484 -0.18992141\n",
      "   0.26658206 -0.36673778 -1.21918848  0.46711487  0.39836931 -0.26412908\n",
      "   1.57993958  0.47605697  0.02195684  0.23291562 -0.4710418   0.34493083\n",
      "  -0.03532108  0.9775651  -0.25979647 -0.34346518 -0.06401293 -0.03752471\n",
      "   0.55780683  0.33649675 -0.2217527  -0.08466947  0.11034143  0.0321694\n",
      "  -0.32275029  0.62176385 -0.92084863  0.74276732 -0.61714697 -0.57638922\n",
      "   0.62137422  0.45338528  0.55987947  0.36838197 -0.06982652  0.15706827\n",
      "  -0.51038298  0.56833505  0.12016919 -0.38963217 -0.04557191  0.25450539]] \n",
      "\n",
      "LAYER 2:\n",
      "[[-0.54481518 -0.44226029 -0.29553329 -0.0135344  -0.3625097  -0.86526128\n",
      "  -0.09254774  0.06561319  0.22822486  0.34082326 -0.2031441   0.37973878\n",
      "  -0.59941979  0.1552371   0.40218366 -0.20580286  0.10301807 -0.12799134\n",
      "   0.40656222 -0.80496523]\n",
      " [-0.39209388 -0.49599092  0.1248573   0.20733073 -0.2605855  -0.85853959\n",
      "  -0.04474818  0.76621562  0.26048524  0.34733534  0.00494511  0.3423892\n",
      "  -0.19856496  0.61573115  0.3780858  -0.15419472 -0.31692817 -0.04900678\n",
      "   0.02693624 -1.07236881]\n",
      " [-0.49421077 -0.60766942  0.23159775 -0.02163563 -0.1517559  -0.58308838\n",
      "  -0.38242884  0.27830251  0.11361168  0.35006703  0.01363309  0.22849773\n",
      "  -0.4634323   0.11826387  0.63448928 -0.37679542  0.01530099 -0.06183037\n",
      "   0.19181735 -0.84323203]] \n",
      "\n",
      "LAYER 3:\n",
      "[[-0.26559418  0.10783406 -0.09285601  0.03809725 -0.12392301 -0.253475\n",
      "   0.04385027  0.04833229]\n",
      " [-0.36530542  0.01952199 -0.1346595  -0.10769186  0.03592254 -0.11314171\n",
      "   0.06279971 -0.06001604]\n",
      " [-0.36759554  0.01355337 -0.11764586 -0.02519627  0.01499878 -0.17640966\n",
      "   0.13952326  0.01219088]] \n",
      "\n",
      "LAYER 4:\n",
      "[[-0.0324633  -0.05539647]\n",
      " [-0.02940647 -0.03581014]\n",
      " [-0.04477748 -0.0583522 ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Seed the generator\n",
    "np.random.seed(0)\n",
    "\n",
    "# Input Data with 4 features\n",
    "X = [\n",
    "     [1.0, 2.0, 3.0, 4.0],\n",
    "     [2.0, 5.0, 1.0, 2.0],\n",
    "     [1.5, 2.7, 3.3, 0.8]\n",
    "    ]\n",
    "\n",
    "\n",
    "# Class Declaration for Layer\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) * 0.10\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Number of inputs for each layer is the number of features in the input\n",
    "layer1 = Layer_Dense(4,60)\n",
    "layer2 = Layer_Dense(60,20)\n",
    "layer3 = Layer_Dense(20,8)\n",
    "layer4 = Layer_Dense(8,2)\n",
    "\n",
    "# Output of each layer is created and assigned as input to next layer\n",
    "layer1.forward(X)\n",
    "layer2.forward(layer1.output)\n",
    "layer3.forward(layer2.output)\n",
    "layer4.forward(layer3.output)\n",
    "\n",
    "# Output of each layer is printed\n",
    "print(\"LAYER 1:\")\n",
    "print(layer1.output,\"\\n\")\n",
    "print(\"LAYER 2:\")\n",
    "print(layer2.output,\"\\n\")\n",
    "print(\"LAYER 3:\")\n",
    "print(layer3.output,\"\\n\")\n",
    "print(\"LAYER 4:\")\n",
    "print(layer4.output,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
